{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "import itertools as it\n",
    "import scipy\n",
    "import itertools\n",
    "import tqdm\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import exp,sqrt,log,log10,sign,power,cosh,sinh,tanh,floor\n",
    "rng = np.random.default_rng(12345)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import NullFormatter, MaxNLocator\n",
    "mpl.rcParams.update({\"font.size\": 12})  #\"text.usetex\": True,})\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to load MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "!mkdir SingleTrain\n",
    "!mkdir BM_RandomSearch\n",
    "!mkdir TestTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original, Y_original = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, parser='liac-arff')\n",
    "print(X_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#    CHOICE OF PARAMETERS      #\n",
    "################################\n",
    "# number of MNIST digits to keep (e.g., Ndigit=3 keeps 0,1,2)\n",
    "Ndigit=3\n",
    "# number of hidden units\n",
    "L = 3\n",
    "# use (+1,-1) if SPINS, otherwise use bits (1,0)\n",
    "SPINS=False\n",
    "# use one-hot encoding in hidden space if POTTS (use only if SPINS=False)\n",
    "POTTS=False\n",
    "\n",
    "dname='DATA/'\n",
    "################################\n",
    "\n",
    "# x_min =0 if bits, x_min=-1 if SPINS\n",
    "# level_gap is the difference in values between the max (1) and the min (x_min)\n",
    "if SPINS:\n",
    "    x_min=-1\n",
    "    level_gap=2.\n",
    "else:\n",
    "    x_min=0\n",
    "    level_gap=1.\n",
    "\n",
    "if POTTS:\n",
    "    str_simul=\"RBM_Potts\"\n",
    "    # in one-hot encoding, number of possible hidden states matches L\n",
    "    Nz=L\n",
    "else:\n",
    "    str_simul=\"RBM\"\n",
    "    # number of possible hidden states: 2**L\n",
    "    Nz=2**L\n",
    "    \n",
    "if POTTS and SPINS: \n",
    "    print(\"\\n\\n>>>>>>>> WARNING:  POTTS and SPINS cannot coexist\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data, and digitalize them two levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_MNIST(x, y=[], z=[], Nex=5, S=1.4, side=0, colors=[]):\n",
    "    \"\"\"Show digits\"\"\"\n",
    "    if side==0: side = int(sqrt(x.shape[1]))\n",
    "    if len(y)<1: y=np.full(Nex,\"\")\n",
    "    colors=np.array(colors)\n",
    "    fig, AX = plt.subplots(1,Nex,figsize=(S*Nex,S))\n",
    "    \n",
    "    for i, img in enumerate(x[:Nex].reshape(Nex, side, side)):\n",
    "        if len(colors)==0: newcmp = \"grey\"\n",
    "        else:\n",
    "            col= colors[0] + (colors[1]-colors[0])*(i+1)/(Nex+1)\n",
    "            newcmp = ListedColormap((col,(1,1,1,1)))\n",
    "        ax=AX[i]\n",
    "        ax.imshow(img, cmap=newcmp)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if len(y)>0: ax.set_title(y[i])\n",
    "        if len(z)>0: ax.set_title(''.join(map(str, z[i])),fontsize=9)\n",
    "    plt.show()\n",
    "            \n",
    "def MNIST_bit(X,side=28,level=0.5):\n",
    "    NX=len(X)\n",
    "    #print(X.shape)\n",
    "    print(f\"dataset with {NX} points, each with {len(X[0])} bits\\n\")\n",
    "    if side==14:\n",
    "        X = np.reshape(X,(NX,28,28))\n",
    "        # new value = average over 4 values in a 2x2 square\n",
    "        Xr = 0.25*(X[:,0::2,0::2]+X[:,1::2,0::2]+X[:,0::2,1::2]+X[:,1::2,1::2])\n",
    "        X  = Xr.reshape(NX,side**2)\n",
    "    #print(X.shape)\n",
    "    # binarize data and then convert it to 1/0 or 1/-1\n",
    "    X = np.where(X/255 > level, 1, x_min)\n",
    "    return X.astype(\"int\")\n",
    "\n",
    "list_10_digits = ('0','1','2','3','4','5','6','7','8','9')\n",
    "list_digits = list_10_digits[:Ndigit]\n",
    "print(list_digits)\n",
    "keep=np.isin(Y_original, list_digits)\n",
    "X_keep,Y=X_original[keep],Y_original[keep]\n",
    "\n",
    "data,label = MNIST_bit(X_keep),Y\n",
    "data,label = data.astype(\"int\"),label.astype(\"int\")\n",
    "print(\"first 10 MNIST data points\")\n",
    "show_MNIST(X_original, Y_original,Nex=10)\n",
    "print(f\"first 10 MNIST-{Ndigit} data points\")\n",
    "show_MNIST(X_keep, label,Nex=10)\n",
    "print(f\"first 10 MNIST-{Ndigit} data points, binarized\")\n",
    "show_MNIST(data, label,Nex=10)\n",
    "\n",
    "# number of data points\n",
    "Nd = len(data)\n",
    "# number of visible units\n",
    "D  = len(data[1])\n",
    "\n",
    "print(f'each of Nd={Nd} data has D={D} bits')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1,1,figsize=(6,3))\n",
    "ax.hist(np.sort(label),bins=np.arange(Ndigit+1)-1/2,density=False,rwidth=0.55,color=\"g\")\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "ax.set_ylabel(\"data points\")\n",
    "plt.show()\n",
    "for i in range(8): show_MNIST(data[i*20:],Nex=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive divergence (CD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eq(213) page 97, activation via sigmoid\n",
    "# taking into account energy gap 2 for \"spin\" variables (-1,1)\n",
    "def CD_step(v_in,wei,bias,details=False,POTTS=False):\n",
    "    \"\"\"\n",
    "        Generates the state on the other layer: \n",
    "        Field \"H\" ->  activation \"a\" -> probability \"p\" -> Spins/bits v_out\n",
    "\n",
    "        Either (v_in=x, wei=w) or (v_in=z, wei=w.T)\n",
    "\n",
    "        details = True --> returns also probability (p) and activation (a) \n",
    "\n",
    "        POTTS means one-hot encoding (used only in hidden space)\n",
    "    \"\"\"\n",
    "    # local \"field\"\n",
    "    H = np.clip(np.dot(v_in, wei) + bias, a_min=-300, a_max=300)\n",
    "    # \"activation\"\n",
    "    a = exp(level_gap*H)\n",
    "    n = np.shape(H)\n",
    "    v_out = np.full(n, x_min, dtype=int) # initially, just a list on -1's or 0's\n",
    "    if POTTS: # RBM with a single hidden unit = 1 (that is, \"one-hot encoding\" with L states)\n",
    "        # p: state probability, normalized to 1 over all units=states\n",
    "        p = a/np.sum(a)\n",
    "        # F: cumulative probability\n",
    "        F = np.cumsum(p)\n",
    "        # pick a state \"i\" randomly with probability p[i]\n",
    "        r = np.random.rand()\n",
    "        i = 0\n",
    "        while r>F[i]: i+=1\n",
    "        v_out[i] = 1 # activate a single hidden unit\n",
    "    else: # normal Ising RBM\n",
    "        # p: local probability, normalized to 1 for each hidden unit\n",
    "        p = a / (a + 1.)\n",
    "        # at each position i, activate the 1's with local probability p[i]\n",
    "        v_out[np.random.random_sample(n) < p] = 1 \n",
    "\n",
    "    if details: return v_out,p,a\n",
    "    else: return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_bias(wE, bE, epoch, L, side=0,cols=0,thr=0,s=1.5, title=False, save=True,cmap=\"bwr\"):\n",
    "    '''\n",
    "    Plot the weights of the RBM, one plot for each hidden unit.\n",
    "    '''\n",
    "    rows = int(np.ceil(L / cols))\n",
    "    if rows==1: rows=2\n",
    "    w=wE[epoch]\n",
    "    b=bE[epoch]\n",
    "    if side==0: side=int(sqrt(len(w)))\n",
    "    if thr==0: thr=4\n",
    "    plt.clf()\n",
    "    fig, AX = plt.subplots(rows, cols+1, figsize=(s*(1+cols),s*rows))\n",
    "    if title: fig.suptitle(f\"epoch = {epoch}\")\n",
    "    k=1\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if rows==1: ax=AX[j+1]\n",
    "            else: ax=AX[i,j+1]\n",
    "            if k<=L:\n",
    "                ax.imshow(w[:,k-1].reshape(side, side), cmap=cmap,vmin=-thr,vmax=thr)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_title(f\"hidden {k}\")\n",
    "            else: fig.delaxes(ax)\n",
    "            k+=1\n",
    "        if i>0:  fig.delaxes(AX[i,0])\n",
    "    \n",
    "    ax=AX[0,0];\n",
    "    im=ax.imshow(b.reshape(side, side), cmap=cmap,vmin=-thr,vmax=thr)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"bias\")\n",
    "    # colobar\n",
    "    cbar_ax = fig.add_axes([0.14, 0.15, 0.024, 0.33])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    S=0.3\n",
    "    plt.subplots_adjust(hspace=S)\n",
    "\n",
    "    if save: plt.savefig(f\"./FIG/FRAME/RBM_{epoch}_w-a.png\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial bias of visible units, based on their average value in the dataset\n",
    "# Hinton, \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "def Hinton_bias_init(x):\n",
    "    xmean=np.array(np.mean(x,axis=0))\n",
    "    # remove values at extrema, to avoid divergences in the log's\n",
    "    S = 1e-4\n",
    "    x1,x2 = x_min+S,1-S\n",
    "    xmean[xmean<x1] = x1\n",
    "    xmean[xmean>x2] = x2\n",
    "    return (1/level_gap)*np.clip(log(xmean-x_min) - log(1-xmean),-300,300)\n",
    "    \n",
    "# range of each initial weight\n",
    "# Glorot and Bengio, \"Understanding the difficulty of training deep feedforward neural networks\"\n",
    "sigma = sqrt(4. / float(L + D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(WhichFolder, WhichData):\n",
    "\n",
    "   data_FileList = glob.glob(f\"{WhichFolder}/{WhichData}_*\") \n",
    "\n",
    "   # Load all files into a dictionary\n",
    "   data = {fname: np.load(fname) for fname in data_FileList}\n",
    "\n",
    "   #return only the values of the dictionary\n",
    "   return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = LoadData('BM_RandomSearch', 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent options\n",
    "GRAD_list=[\"SGD\",\"RMSprop\"]\n",
    "GRAD=GRAD_list[1]\n",
    "if GRAD==\"SGD\":\n",
    "    l_rate_ini,l_rate_fin=1.0, 0.25\n",
    "if GRAD==\"RMSprop\":\n",
    "    beta,epsilon=0.9,1e-4\n",
    "    l_rate_ini,l_rate_fin=0.05, 0.05\n",
    "    print(\"epsilon=\",epsilon)\n",
    "gamma = 0.001 ######### for regularization\n",
    "\n",
    "print(f\"D={D}\\tsample size\\nL={L}\\tnr. z states\")\n",
    "print(\"Gradient descent type:\",GRAD)\n",
    "print(f\"learning rate        = {l_rate_ini} --> {l_rate_fin}\")\n",
    "if gamma!=0: print(f\"gamma={gamma}\\tregularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FileName(label, time, hp, FolderName):\n",
    "   #Label might be: a, b or w, logL\n",
    "   #hyperparam: [#-if-digits, #-of-hidden-var, CDnumber, optimizr, l_rate_in, l_rate_fin, epochs, gamma, POTTS, SPINS ]\n",
    "\n",
    "   fname = f'{FolderName}/{label}_RBM_{hp['Ndigit']}_{hp['L']}_{hp['Nt']}_{hp['Opt']}_{hp['l_rate_in']:.2f}-{hp['l_rate_fin']:.2f}_Ep{hp['epochs']}_Mini{hp['mini']}'\n",
    "   if hp['gamma'] > 0: fname = fname + f'_reg{hp['gamma']:.3f}'\n",
    "   if hp['POTTS']: fname = fname + f'_POTTS'\n",
    "   if hp['SPINS']: fname = fname + f'_SPINS'\n",
    "   fname = fname + time + '.npy'\n",
    "\n",
    "   return fname\n",
    "\n",
    "def SaveFile(label, time, hp, data, FolderName):\n",
    "   fname = FileName(label, time, hp, FolderName)\n",
    "\n",
    "   np.save(fname, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log-likelihood $\\mathcal{L}$ function\n",
    "\n",
    "Allow to compute the log likelihood with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(a, b, w, data):\n",
    "    \n",
    "    Z_a = []\n",
    "    exp_data_dip = []\n",
    "    L = len(b)\n",
    "    D = len(data[0])\n",
    "    q = 1.4\n",
    "    all_z = list(itertools.product([0, 1], repeat=L))\n",
    "    for z in all_z:\n",
    "        G = np.prod([np.exp(z[i]*b[i]) for i in range(len(b))])\n",
    "        H = a + np.dot(w,z) \n",
    "        #q = np.mean(np.ones(len(H))+np.exp(H))\n",
    "    \n",
    "        Z_tilde = G*np.prod((np.ones(len(H))+np.exp(H))/q)\n",
    "        Z_a.append(Z_tilde)\n",
    "\n",
    "        #Ora voglio ciclare sulle x dei dati (i primi 1000 dati)\n",
    "        s_prod_exp_tot = 0\n",
    "        for ics in data:\n",
    "            s_prod_exp = H.dot(ics)\n",
    "            s_prod_exp_tot += s_prod_exp\n",
    "        exp_data_dip.append(s_prod_exp_tot)\n",
    "\n",
    "    #Z\n",
    "    Z = np.sum(Z_a)\n",
    "    log_Z = np.log(Z)+D*np.log(q)\n",
    "    #log_first_part = np.log(np.sum(np.exp(exp_data_dip)))\n",
    "    log_first_part = scipy.special.logsumexp(exp_data_dip)\n",
    "    \n",
    "    log_L = (log_first_part - log_Z)/D\n",
    "\n",
    "    return log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM train function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultHyperp = {'Ndigit': 3, 'L': 3, 'Nt': 2, 'Opt': 'RMSprop', 'l_rate_in': 0.05, 'l_rate_fin': 0.05, 'epochs': 150, 'mini': 20, 'gamma': 0.001, 'POTTS': False, 'SPINS': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'FolderName' is the folder in which we want to save the file: SingleTrain for this, BM_RandomSearch for the random search\n",
    "def train_RBM(Hyperparams, FolderName, Debug = False, LL_Plot = True, ReturnLL = False): \n",
    "\n",
    "   #Only to check if the directory FolderName exist\n",
    "   if os.path.isdir(FolderName):\n",
    "      print(\"Directory exists\")\n",
    "   \n",
    "      L = Hyperparams['L']\n",
    "      Nepoch = Hyperparams['epochs']\n",
    "      Nt = Hyperparams['Nt']\n",
    "      gamma = Hyperparams['gamma']\n",
    "      l_rate_ini, l_rate_fin = Hyperparams['l_rate_in'], Hyperparams['l_rate_fin']\n",
    "      GRAD = Hyperparams['Opt']\n",
    "      POTTS, SPINS = Hyperparams['POTTS'], Hyperparams['SPINS']\n",
    "\n",
    "      # minibatches per epoch\n",
    "      Nmini = Hyperparams['mini']\n",
    "      # minibatch size at initial epoch and final one\n",
    "      N_ini, N_fin= 10, 400\n",
    "\n",
    "      print(Hyperparams)\n",
    "\n",
    "      # initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "      w = sigma * np.random.randn(D,L)\n",
    "      #a = sigma * np.random.randn(D)\n",
    "      # using Hinton initialization of visible biases\n",
    "      a = Hinton_bias_init(data)\n",
    "      # hidden biases initialized to zero\n",
    "      b = np.zeros(L)\n",
    "\n",
    "      if Debug: print(f\"Nepoch={Nepoch}\\nNmini={Nmini}\")\n",
    "\n",
    "      # recording history of weights (\"E\" means epoch)\n",
    "      wE,aE,bE=np.zeros((Nepoch+1,D,L)),np.zeros((Nepoch+1,D)),np.zeros((Nepoch+1,L))\n",
    "      wE[0],aE[0],bE[0]=np.copy(w),np.copy(a),np.copy(b)\n",
    "      gwE,gwE_d,gwE_m = np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE)\n",
    "      gaE,gaE_d,gaE_m = np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE)\n",
    "      gbE,gbE_d,gbE_m = np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE)\n",
    "      miniE = np.zeros(Nepoch+1)\n",
    "      pzE=np.zeros((Nepoch+1,Nz))\n",
    "      \n",
    "      log_L_list = []\n",
    "\n",
    "\n",
    "      if GRAD==\"RMSprop\": \n",
    "         gw2,ga2,gb2 = np.zeros_like(w),np.zeros_like(a),np.zeros_like(b)\n",
    "\n",
    "      indices=np.arange(Nd).astype(\"int\")\n",
    "\n",
    "      if Debug:\n",
    "         plot_weights_bias(wE, aE, 0, L, cols=6, save=False)\n",
    "\n",
    "      # for the plot with panels\n",
    "      Ncols=min(8,max(2,L//2))\n",
    "\n",
    "      if Debug:\n",
    "         if POTTS: print(\"Starting the training, POTTS=True\")\n",
    "         else: print(\"Starting the training\")\n",
    "\n",
    "      # Note: here an epoch does not analyze the whole dataset\n",
    "      for epoch in tqdm.tqdm(range(1,Nepoch+1)):\n",
    "         # q maps epochs to interval [0,1]\n",
    "         q = (epoch-1.)/(Nepoch-1.) \n",
    "         # N, size of the mini batch\n",
    "         # stays closer to N_ini for some time, then it progressively accelerates toward N_fin\n",
    "         N = int(N_ini + (N_fin-N_ini)*(q**2))\n",
    "         #  l_rate interpolates between initial and final value\n",
    "         l_rate = l_rate_ini + (l_rate_fin-l_rate_ini)*q\n",
    "\n",
    "         selected = np.random.choice(indices, N, replace=False)\n",
    "\n",
    "         for mini in range(Nmini):\n",
    "            # initializitation for averages in minibatch\n",
    "            # visible variables \"v\" --> \"x\"\n",
    "            #  hidden variables \"h\" --> \"z\"\n",
    "            x_data, x_model = np.zeros(D),np.zeros(D)\n",
    "            z_data, z_model = np.zeros(L),np.zeros(L)\n",
    "            xz_data,xz_model= np.zeros((D,L)),np.zeros((D,L))\n",
    "            pz = np.zeros(L)\n",
    "            \n",
    "            # Minibatch of size N: points randomply picked (without repetition) from data\n",
    "            #if Debug:\n",
    "               #if epoch==1 and mini<=3: print(selected)\n",
    "\n",
    "            for k in range(N):\n",
    "               ###################################\n",
    "               x0 = data[selected[k]]\n",
    "               # positive CD phase: generating z from x[k]\n",
    "               z = CD_step(x0,w,b,POTTS=POTTS)\n",
    "               x_data  += x0\n",
    "               z_data  += z\n",
    "               xz_data += np.outer(x0,z)\n",
    "               # fantasy\n",
    "               zf=np.copy(z)\n",
    "               # Contrastive divergence with Nt steps\n",
    "               for t in range(Nt):\n",
    "                  # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "                  xf = CD_step(zf,w.T,a)\n",
    "                  # positive CD phase: generating fantasy zf from fantasy xf \n",
    "                  zf = CD_step(xf,w,b,POTTS=POTTS)\n",
    "               x_model += xf\n",
    "               z_model += zf\n",
    "               xz_model+= np.outer(xf,zf)\n",
    "               # recording probability of encoding in z-space, if POTTS\n",
    "               if POTTS: pz[zf]+=1\n",
    "               ###################################\n",
    "            \n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            gw_d,gw_m = xz_data/N, xz_model/N\n",
    "            ga_d,ga_m = x_data/N, x_model/N\n",
    "            gb_d,gb_m = z_data/N, z_model/N\n",
    "            gw=np.copy(gw_d - gw_m)\n",
    "            ga=np.copy(ga_d - ga_m)\n",
    "            gb=np.copy(gb_d - gb_m)\n",
    "\n",
    "            # gradient ascent step\n",
    "            if GRAD==\"RMSprop\":\n",
    "               # RMSprop gradient ascent\n",
    "               gw2 = beta*gw2+(1-beta)*np.square(gw)\n",
    "               ga2 = beta*ga2+(1-beta)*np.square(ga)\n",
    "               gb2 = beta*gb2+(1-beta)*np.square(gb)\n",
    "               w += l_rate*gw/sqrt(epsilon+gw2)\n",
    "               a += l_rate*ga/sqrt(epsilon+ga2)\n",
    "               b += l_rate*gb/sqrt(epsilon+gb2)\n",
    "            else: \n",
    "                  # defaulting to the vanilla stochastic gradient ascent (SGD)\n",
    "               w += l_rate*gw\n",
    "               a += l_rate*ga\n",
    "               b += l_rate*gb\n",
    "            # regularization (LASSO)\n",
    "            if gamma>0.:\n",
    "               w -= (gamma*l_rate)*sign(w)\n",
    "               a -= (gamma*l_rate)*sign(a)\n",
    "               b -= (gamma*l_rate)*sign(b)\n",
    "\n",
    "         wE[epoch],gwE[epoch],gwE_d[epoch],gwE_m[epoch]=np.copy(w),np.copy(gw),np.copy(gw_d),np.copy(gw_m)\n",
    "         aE[epoch],gaE[epoch],gaE_d[epoch],gaE_m[epoch]=np.copy(a),np.copy(ga),np.copy(ga_d),np.copy(ga_m)\n",
    "         bE[epoch],gbE[epoch],gbE_d[epoch],gbE_m[epoch]=np.copy(b),np.copy(gb),np.copy(gb_d),np.copy(gb_m)\n",
    "         miniE[epoch]=N\n",
    "\n",
    "         #Computing the likelihood\n",
    "         log_L = log_likelihood(a, b, w, data)\n",
    "         log_L_list.append(log_L)\n",
    "\n",
    "         if POTTS: pzE[epoch] = pz/np.sum(pz)\n",
    "         # if Debug: \n",
    "         #    print(\"epoch\",epoch,\"/\",Nepoch,\" Nt:\",Nt,\" N:\",N,\" L:\",L,\n",
    "         #          \" rate:\",l_rate,\" gam:\",gamma,\"SPINS=\",SPINS,\"POTTS=\",POTTS)\n",
    "      \n",
    "         str_time = datetime.datetime.now().strftime(\"_%m%d_%H%M\")\n",
    "\n",
    "      if Debug:\n",
    "         plot_weights_bias(wE, aE, epoch, L, cols=Ncols, save=False)\n",
    "\n",
    "      SaveFile('w', str_time, Hyperparams, wE, FolderName)\n",
    "      SaveFile('b', str_time, Hyperparams, bE, FolderName)\n",
    "      SaveFile('a', str_time, Hyperparams, aE, FolderName)\n",
    "      SaveFile('logL', str_time, Hyperparams, log_L_list, FolderName)\n",
    "\n",
    "\n",
    "      \n",
    "      if Debug: print(\"END of learning phase\")\n",
    "\n",
    "      if LL_Plot:\n",
    "         plt.plot(range(Nepoch), log_L_list)\n",
    "         plt.title(\"log-Likehood trend\")  # Titolo\n",
    "         plt.xlabel(\"# epoch\")  # Etichetta asse X\n",
    "         plt.ylabel(\"log-L\")  # Etichetta asse Y\n",
    "         plt.grid(True)  # Aggiunge una griglia\n",
    "         plt.show()\n",
    "\n",
    "      if ReturnLL: \n",
    "         return np.mean(log_L_list[-50:])\n",
    "\n",
    "   else:\n",
    "      print(\"Directory does not exist\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RBM(DefaultHyperp, FolderName = 'SingleTrain', Debug = True, LL_Plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify how $\\mathcal{L}$ changes with L and CD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TestHyperp = {'Ndigit': 3, 'L': 3, 'Nt': 2, \n",
    "#                  'Opt': 'RMSprop', 'l_rate_in': 0.05, 'l_rate_fin': 0.05, 'epochs': 200, 'mini': 20, 'gamma': 0.001, 'POTTS': False, 'SPINS': False}\n",
    "\n",
    "# L_range = [1, 2, 3, 4, 5]\n",
    "# Nt_range = [2, 3, 4, 5]\n",
    "\n",
    "# TestLogLik = np.zeros((len(L_range), len(Nt_range)))\n",
    "\n",
    "# for i, l in enumerate(L_range):\n",
    "#    for j, nt in enumerate(Nt_range):\n",
    "#       TestHyperp.update({'L': l, 'Nt': nt})\n",
    "#       LLMean = train_RBM(TestHyperp, 'TestTrain', Debug = False, LL_Plot = False, ReturnLL = True)\n",
    "#       TestLogLik[i][j] = LLMean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(TestLogLik)\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.colorbar()\n",
    "# plt.xticks(ticks=np.arange(0, len(Nt_range)), labels=np.arange(2, len(L_range) + 1))\n",
    "# plt.yticks(ticks=np.arange(0, len(L_range)), labels=np.arange(1, len(L_range) + 1))\n",
    "# plt.xlabel(\"# Contrastive Divergence\")\n",
    "# plt.ylabel(\"# Hidden Layers\")\n",
    "\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP = {'L': [3, 8], \n",
    "      'Nt': [2, 6], \n",
    "      'Opt': ['RMSprop', 'SGD'],\n",
    "      'l_rate_in': [1, 0.1], \n",
    "      'l_rate_fin': [0.1, 0.05], \n",
    "      'epochs': [100, 251], \n",
    "      'mini': [10, 51],\n",
    "      'gamma': [0.1, 0.001], \n",
    "      #'POTTS': [False, True], \n",
    "      #'SPINS': False\n",
    "      }\n",
    "\n",
    "\n",
    "def RandomSearch(HP, Ndigit = 3, nModels = 10):\n",
    "\n",
    "   HyperParams_List = []\n",
    "\n",
    "   for _ in range(nModels):\n",
    "\n",
    "      L, Nt = np.random.randint(HP['L'][0], HP['L'][1]), np.random.randint(HP['Nt'][0], HP['Nt'][1] )\n",
    "      l_rate_in = np.random.uniform(HP['l_rate_in'][0], HP['l_rate_in'][1])\n",
    "      l_rate_fin = np.random.uniform(HP['l_rate_fin'][0], HP['l_rate_fin'][1])\n",
    "      epochs, mini = np.random.choice(range(HP['epochs'][0], HP['epochs'][1]), 15), np.random.choice(range(HP['mini'][0], HP['mini'][1]), 5)\n",
    "      gamma = np.random.uniform(HP['gamma'][0], HP['gamma'][1])\n",
    "\n",
    "      Opt = np.random.choice(HP['Opt'])\n",
    "\n",
    "      HyperParams = {'Ndigit': Ndigit, 'L': L, 'Nt': Nt, 'Opt': Opt, 'l_rate_in': l_rate_in, \n",
    "                     'l_rate_fin': l_rate_fin, 'epochs': epochs, 'mini': mini, 'gamma': gamma, 'POTTS': False, 'SPINS': False}\n",
    "\n",
    "      HyperParams_List.append(HyperParams)\n",
    "\n",
    "   return HyperParams_List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParam_List = RandomSearch(HP, nModels = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with all the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for HP in HyperParam_List:\n",
    "   train_RBM(HP, FolderName = 'BM_RandomSearch', Debug = False, LL_Plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generative power of the trained RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aE = np.load('WEIGHTS/a_RBM_3_3_2_RMSprop_0.05-0.05_Ep150_reg0.001_0312_1502.npy')\n",
    "#bE = np.load('WEIGHTS/b_RBM_3_3_2_RMSprop_0.05-0.05_Ep150_reg0.001_0312_1502.npy')\n",
    "#wE = np.load('WEIGHTS/w_RBM_3_3_2_RMSprop_0.05-0.05_Ep150_reg0.001_0312_1502.npy')\n",
    "\n",
    "ee=-1\n",
    "NN=200\n",
    "traj_x,traj_z = np.zeros((NN+2,D)), np.zeros((NN+2,L))\n",
    "xf=np.copy(data[np.random.randint(Nd)])\n",
    "traj_x[0]=np.copy(xf)\n",
    "\n",
    "# AF: multiply weights and biases by a number >1 to achieve a more deterministic behavior,\n",
    "# equivalent to lower the temperature in a Boltzmann weight -> select lowest energies\n",
    "# Note: here, this is done in the negative CD step only\n",
    "AF=10.\n",
    "\n",
    "for t in range(NN):\n",
    "    t1=t+1\n",
    "    # positive CD phase: generating fantasy zf from fantasy xf \n",
    "    zf = CD_step(xf,1*wE[ee],1*bE[ee],POTTS=POTTS)\n",
    "    traj_z[t] = np.copy(zf)\n",
    "    # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "    xf = CD_step(zf,AF*wE[ee].T,AF*aE[ee])\n",
    "    traj_x[t1] = np.copy(xf)\n",
    "\n",
    "\n",
    "plot_weights_bias(wE, aE, 150, L, cols=3, save=False)\n",
    "\n",
    "col0,col1,col2,col3,col4=(0.8,0,0,1),(1,0.6,0,1),(0,.7,0,1),(0.2,0.2,1,1),(1,0,1,1)\n",
    "show_MNIST(traj_x[0:],Nex=10,colors=(col0,col1))\n",
    "show_MNIST(traj_x[10:],Nex=10,colors=(col1,col2))\n",
    "show_MNIST(traj_x[20:],Nex=10,colors=(col2,col3))\n",
    "show_MNIST(traj_x[40:],Nex=10,colors=(col3,col4))\n",
    "print(\"L:\",L,\"    amplification of weights:\",AF )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the gradient amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fname(strin):\n",
    "    fname=\"FIG/RBM_\"+strin+\"_MNIST\"+str(Ndigit)+\"_E\"+str(NE)+\"_N\"+str(N_ini)+\"-\"+str(N_fin)\n",
    "    fname=fname+\"_\"+GRAD+\"-\"+str(l_rate_ini)+\"-\"+str(l_rate_fin)+\"_CD-\"+str(Nt)+\"_L\"+str(L)\n",
    "    if gamma>0:  fname=fname+\"_reg\"+str(gamma)\n",
    "    return fname+\".png\"\n",
    "\n",
    "NE = len(gwE)-1\n",
    "print(\"NE=\",NE)\n",
    "mgw,mga,mgb,epo=np.zeros(NE),np.zeros(NE),np.zeros(NE),np.zeros(NE)\n",
    "DE=1\n",
    "if NE>30: DE=2\n",
    "if NE>50: DE=5\n",
    "if NE>100: DE=10\n",
    "# compute RMS average of gradients during epochs\n",
    "for ep in range(1,1+NE):\n",
    "    epo[ep-1]=ep\n",
    "    mgw[ep-1] = np.std(gwE[ep])\n",
    "    mga[ep-1] = np.std(gaE[ep])\n",
    "    mgb[ep-1] = np.std(gbE[ep])\n",
    "\n",
    "#############################################################################\n",
    "# creating grid for subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,5.5))\n",
    "fig.subplots_adjust( left=None, bottom=None,  right=None, top=None, wspace=None, hspace=None)\n",
    "        \n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(DE))\n",
    "ax.plot(epo,mgw,\"o-\",c=\"#CC77EC\",label=\"$w$\",zorder=10)\n",
    "ax.plot(epo,mga,\"d--\",c=\"#22C040\",label=\"$a$\",zorder=5)\n",
    "ax.plot(epo,mgb,\"^:\",c=\"#EEC006\",label=\"$b$\",zorder=8)\n",
    "ax.plot(epo,.35/sqrt(miniE[1:]),\"k-.\",label=\"$\\sim 1 / \\sqrt{N}$\",zorder=20,lw=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"gradient RMS\")\n",
    "ax.set_xlim(0.5,NE+0.5)\n",
    "if 1==2: ax.set_ylim(0,)\n",
    "else: \n",
    "    ax.set_xlim(0.8,Nepoch*1.1)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "fname=make_fname(\"grad\")\n",
    "fig.savefig(fname,dpi=200,bbox_inches='tight',pad_inches=0.02, transparent=False)\n",
    "plt.show()\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# codice di Marco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wE[Nepoch]\n",
    "a = aE[epoch]\n",
    "b = bE[epoch]\n",
    "\n",
    "selectA = 1\n",
    "selectB = 2\n",
    "index_selectA = np.random.choice( np.where(label == selectA)[0] )\n",
    "index_selectB = np.random.choice( np.where(label == selectB)[0] )\n",
    "dataA = data[index_selectA]\n",
    "dataB = data[index_selectB]\n",
    "\n",
    "zA = CD_step(dataA,W,b)\n",
    "zB = CD_step(dataB,W,b)\n",
    "\n",
    "def energy(x, z, W, a, b):\n",
    "    return -(np.dot(x, a) + np.dot(z, b) + np.dot(x, np.dot(W , z)))\n",
    "\n",
    "n_steps = 100\n",
    "energies = []\n",
    "intermediate_images = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, n_steps):\n",
    "    # PART OVERCOME BARRIER\n",
    "    '''\n",
    "    x_intermediate = np.zeros(D)\n",
    "    #ora per ogni pixel, decido con la soglia random se prendere il valore da dataA o da dataB\n",
    "    for i in range(D):\n",
    "        if np.random.uniform() > alpha:\n",
    "            x_intermediate[i] = dataA[i]\n",
    "        else:\n",
    "            x_intermediate[i] = dataB[i]\n",
    "    '''\n",
    "    # ----------------------\n",
    "    # PART SHOW BARRIER\n",
    "    x_intermediate = (1 - alpha) * dataA + alpha * dataB  # Interpolazione lineare\n",
    "    x_intermediate = (x_intermediate > 0.5).astype(int)  # Soglia binaria\n",
    "    z_intermediate = CD_step(x_intermediate, W, b,)\n",
    "    # ----------------------\n",
    "    E = energy(x_intermediate, z_intermediate, W, a, b)\n",
    "    energies.append(E)\n",
    "    intermediate_images.append(x_intermediate.reshape(28, 28))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(n_steps), energies, marker='o')\n",
    "plt.xlabel(f\"Linear transition between {selectA} and {selectB}\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.title(f\"Energetic barrier between {selectA} and {selectB}\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# show some intermediate step\n",
    "show_inter_steps = 5\n",
    "fig, axes = plt.subplots(1, show_inter_steps, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(intermediate_images[i * (n_steps // show_inter_steps)], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = wE[Nepoch]\n",
    "a = aE[epoch]\n",
    "b = bE[epoch]\n",
    "\n",
    "selectA = 1\n",
    "selectB = 2\n",
    "index_selectA = np.random.choice( np.where(label == selectA)[0] )\n",
    "index_selectB = np.random.choice( np.where(label == selectB)[0] )\n",
    "dataA = data[index_selectA]\n",
    "dataB = data[index_selectB]\n",
    "\n",
    "zA = CD_step(dataA,W,b)\n",
    "zB = CD_step(dataB,W,b)\n",
    "\n",
    "def energy(x, z, W, a, b):\n",
    "    return -(np.dot(x, a) + np.dot(z, b) + np.dot(x, np.dot(W , z)))\n",
    "\n",
    "n_steps = 100\n",
    "energies = []\n",
    "intermediate_images = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, n_steps):\n",
    "    # PART OVERCOME BARRIER\n",
    "    x_intermediate = np.zeros(D)\n",
    "    #ora per ogni pixel, decido con la soglia random se prendere il valore da dataA o da dataB\n",
    "    for i in range(D):\n",
    "        if np.random.uniform() > alpha:\n",
    "            x_intermediate[i] = dataA[i]\n",
    "        else:\n",
    "            x_intermediate[i] = dataB[i]\n",
    "    # ----------------------\n",
    "    '''\n",
    "    # PART SHOW BARRIER\n",
    "    x_intermediate = (1 - alpha) * dataA + alpha * dataB  # Interpolazione lineare\n",
    "    x_intermediate = (x_intermediate > 0.5).astype(int)  # Soglia binaria\n",
    "    z_intermediate = CD_step(x_intermediate, W, b,)\n",
    "    # ----------------------\n",
    "    '''\n",
    "    E = energy(x_intermediate, z_intermediate, W, a, b)\n",
    "    energies.append(E)\n",
    "    intermediate_images.append(x_intermediate.reshape(28, 28))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(range(n_steps), energies, marker='o')\n",
    "plt.xlabel(f\"Random transition between {selectA} and {selectB}\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.title(f\"Energetic barrier between {selectA} and {selectB}\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# show some intermediate step\n",
    "show_inter_steps = 5\n",
    "fig, axes = plt.subplots(1, show_inter_steps, figsize=(10, 2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(intermediate_images[i * (n_steps // show_inter_steps)], cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
