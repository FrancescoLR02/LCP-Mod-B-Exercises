{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine - MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, datetime\n",
    "import itertools as it\n",
    "import scipy\n",
    "import itertools\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import exp,sqrt,log,log10,sign,power,cosh,sinh,tanh,floor\n",
    "rng = np.random.default_rng(12345)\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import NullFormatter, MaxNLocator\n",
    "mpl.rcParams.update({\"font.size\": 12})  #\"text.usetex\": True,})\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to load MNIST\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "!mkdir FIG \n",
    "!mkdir FIG/FRAME\n",
    "!mkdir WEIGHTS \n",
    "!mkdir DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_original, Y_original = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False, parser='liac-arff')\n",
    "print(X_original.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "#    CHOICE OF PARAMETERS      #\n",
    "################################\n",
    "# number of MNIST digits to keep (e.g., Ndigit=3 keeps 0,1,2)\n",
    "Ndigit=3\n",
    "# number of hidden units\n",
    "L = 3\n",
    "# use (+1,-1) if SPINS, otherwise use bits (1,0)\n",
    "SPINS=False\n",
    "# use one-hot encoding in hidden space if POTTS (use only if SPINS=False)\n",
    "POTTS=False\n",
    "\n",
    "dname='DATA/'\n",
    "################################\n",
    "\n",
    "# x_min =0 if bits, x_min=-1 if SPINS\n",
    "# level_gap is the difference in values between the max (1) and the min (x_min)\n",
    "if SPINS:\n",
    "    x_min=-1\n",
    "    level_gap=2.\n",
    "else:\n",
    "    x_min=0\n",
    "    level_gap=1.\n",
    "\n",
    "if POTTS:\n",
    "    str_simul=\"RBM_Potts\"\n",
    "    # in one-hot encoding, number of possible hidden states matches L\n",
    "    Nz=L\n",
    "else:\n",
    "    str_simul=\"RBM\"\n",
    "    # number of possible hidden states: 2**L\n",
    "    Nz=2**L\n",
    "    \n",
    "if POTTS and SPINS: \n",
    "    print(\"\\n\\n>>>>>>>> WARNING:  POTTS and SPINS cannot coexist\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data, and digitalize them two levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_MNIST(x, y=[], z=[], Nex=5, S=1.4, side=0, colors=[]):\n",
    "    \"\"\"Show digits\"\"\"\n",
    "    if side==0: side = int(sqrt(x.shape[1]))\n",
    "    if len(y)<1: y=np.full(Nex,\"\")\n",
    "    colors=np.array(colors)\n",
    "    fig, AX = plt.subplots(1,Nex,figsize=(S*Nex,S))\n",
    "    \n",
    "    for i, img in enumerate(x[:Nex].reshape(Nex, side, side)):\n",
    "        if len(colors)==0: newcmp = \"grey\"\n",
    "        else:\n",
    "            col= colors[0] + (colors[1]-colors[0])*(i+1)/(Nex+1)\n",
    "            newcmp = ListedColormap((col,(1,1,1,1)))\n",
    "        ax=AX[i]\n",
    "        ax.imshow(img, cmap=newcmp)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if len(y)>0: ax.set_title(y[i])\n",
    "        if len(z)>0: ax.set_title(''.join(map(str, z[i])),fontsize=9)\n",
    "    plt.show()\n",
    "            \n",
    "def MNIST_bit(X,side=28,level=0.5):\n",
    "    NX=len(X)\n",
    "    #print(X.shape)\n",
    "    print(f\"dataset with {NX} points, each with {len(X[0])} bits\\n\")\n",
    "    if side==14:\n",
    "        X = np.reshape(X,(NX,28,28))\n",
    "        # new value = average over 4 values in a 2x2 square\n",
    "        Xr = 0.25*(X[:,0::2,0::2]+X[:,1::2,0::2]+X[:,0::2,1::2]+X[:,1::2,1::2])\n",
    "        X  = Xr.reshape(NX,side**2)\n",
    "    #print(X.shape)\n",
    "    # binarize data and then convert it to 1/0 or 1/-1\n",
    "    X = np.where(X/255 > level, 1, x_min)\n",
    "    return X.astype(\"int\")\n",
    "\n",
    "list_10_digits = ('0','1','2','3','4','5','6','7','8','9')\n",
    "list_digits = list_10_digits[:Ndigit]\n",
    "print(list_digits)\n",
    "keep=np.isin(Y_original, list_digits)\n",
    "X_keep,Y=X_original[keep],Y_original[keep]\n",
    "\n",
    "data,label = MNIST_bit(X_keep),Y\n",
    "data,label = data.astype(\"int\"),label.astype(\"int\")\n",
    "print(\"first 10 MNIST data points\")\n",
    "show_MNIST(X_original, Y_original,Nex=10)\n",
    "print(f\"first 10 MNIST-{Ndigit} data points\")\n",
    "show_MNIST(X_keep, label,Nex=10)\n",
    "print(f\"first 10 MNIST-{Ndigit} data points, binarized\")\n",
    "show_MNIST(data, label,Nex=10)\n",
    "\n",
    "# number of data points\n",
    "Nd = len(data)\n",
    "# number of visible units\n",
    "D  = len(data[1])\n",
    "\n",
    "print(f'each of Nd={Nd} data has D={D} bits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax) = plt.subplots(1,1,figsize=(6,3))\n",
    "ax.hist(np.sort(label),bins=np.arange(Ndigit+1)-1/2,density=False,rwidth=0.55,color=\"g\")\n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(1))\n",
    "ax.set_ylabel(\"data points\")\n",
    "plt.show()\n",
    "for i in range(8): show_MNIST(data[i*20:],Nex=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive divergence (CD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eq(213) page 97, activation via sigmoid\n",
    "# taking into account energy gap 2 for \"spin\" variables (-1,1)\n",
    "def CD_step(v_in,wei,bias,details=False,POTTS=False):\n",
    "    \"\"\"\n",
    "        Generates the state on the other layer: \n",
    "        Field \"H\" ->  activation \"a\" -> probability \"p\" -> Spins/bits v_out\n",
    "\n",
    "        Either (v_in=x, wei=w) or (v_in=z, wei=w.T)\n",
    "\n",
    "        details = True --> returns also probability (p) and activation (a) \n",
    "\n",
    "        POTTS means one-hot encoding (used only in hidden space)\n",
    "    \"\"\"\n",
    "    # local \"field\"\n",
    "    H = np.clip(np.dot(v_in, wei) + bias, a_min=-300, a_max=300)\n",
    "    # \"activation\"\n",
    "    a = exp(level_gap*H)\n",
    "    n = np.shape(H)\n",
    "    v_out = np.full(n, x_min, dtype=int) # initially, just a list on -1's or 0's\n",
    "    if POTTS: # RBM with a single hidden unit = 1 (that is, \"one-hot encoding\" with L states)\n",
    "        # p: state probability, normalized to 1 over all units=states\n",
    "        p = a/np.sum(a)\n",
    "        # F: cumulative probability\n",
    "        F = np.cumsum(p)\n",
    "        # pick a state \"i\" randomly with probability p[i]\n",
    "        r = np.random.rand()\n",
    "        i = 0\n",
    "        while r>F[i]: i+=1\n",
    "        v_out[i] = 1 # activate a single hidden unit\n",
    "    else: # normal Ising RBM\n",
    "        # p: local probability, normalized to 1 for each hidden unit\n",
    "        p = a / (a + 1.)\n",
    "        # at each position i, activate the 1's with local probability p[i]\n",
    "        v_out[np.random.random_sample(n) < p] = 1 \n",
    "\n",
    "    if details: return v_out,p,a\n",
    "    else: return v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_bias(wE, bE, epoch, L, side=0,cols=0,thr=0,s=1.5, title=False, save=True,cmap=\"bwr\"):\n",
    "    '''\n",
    "    Plot the weights of the RBM, one plot for each hidden unit.\n",
    "    '''\n",
    "    rows = int(np.ceil(L / cols))\n",
    "    if rows==1: rows=2\n",
    "    w=wE[epoch]\n",
    "    b=bE[epoch]\n",
    "    if side==0: side=int(sqrt(len(w)))\n",
    "    if thr==0: thr=4\n",
    "    plt.clf()\n",
    "    fig, AX = plt.subplots(rows, cols+1, figsize=(s*(1+cols),s*rows))\n",
    "    if title: fig.suptitle(f\"epoch = {epoch}\")\n",
    "    k=1\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if rows==1: ax=AX[j+1]\n",
    "            else: ax=AX[i,j+1]\n",
    "            if k<=L:\n",
    "                ax.imshow(w[:,k-1].reshape(side, side), cmap=cmap,vmin=-thr,vmax=thr)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_title(f\"hidden {k}\")\n",
    "            else: fig.delaxes(ax)\n",
    "            k+=1\n",
    "        if i>0:  fig.delaxes(AX[i,0])\n",
    "    \n",
    "    ax=AX[0,0];\n",
    "    im=ax.imshow(b.reshape(side, side), cmap=cmap,vmin=-thr,vmax=thr)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(\"bias\")\n",
    "    # colobar\n",
    "    cbar_ax = fig.add_axes([0.14, 0.15, 0.024, 0.33])\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax)\n",
    "    cbar.ax.tick_params(labelsize=12)\n",
    "    \n",
    "    S=0.3\n",
    "    plt.subplots_adjust(hspace=S)\n",
    "\n",
    "    if save: plt.savefig(f\"./FIG/FRAME/RBM_{epoch}_w-a.png\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial bias of visible units, based on their average value in the dataset\n",
    "# Hinton, \"A Practical Guide to Training Restricted Boltzmann Machines\"\n",
    "def Hinton_bias_init(x):\n",
    "    xmean=np.array(np.mean(x,axis=0))\n",
    "    # remove values at extrema, to avoid divergences in the log's\n",
    "    S = 1e-4\n",
    "    x1,x2 = x_min+S,1-S\n",
    "    xmean[xmean<x1] = x1\n",
    "    xmean[xmean>x2] = x2\n",
    "    return (1/level_gap)*np.clip(log(xmean-x_min) - log(1-xmean),-300,300)\n",
    "    \n",
    "# range of each initial weight\n",
    "# Glorot and Bengio, \"Understanding the difficulty of training deep feedforward neural networks\"\n",
    "sigma = sqrt(4. / float(L + D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent options\n",
    "GRAD_list=[\"SGD\",\"RMSprop\"]\n",
    "GRAD=GRAD_list[1]\n",
    "if GRAD==\"SGD\":\n",
    "    l_rate_ini,l_rate_fin=1.0,0.25\n",
    "if GRAD==\"RMSprop\":\n",
    "    beta,epsilon=0.9,1e-4\n",
    "    l_rate_ini,l_rate_fin=0.05,0.05\n",
    "    print(\"epsilon=\",epsilon)\n",
    "gamma = 0.001 ######### for regularization\n",
    "\n",
    "print(f\"D={D}\\tsample size\\nL={L}\\tnr. z states\")\n",
    "print(\"Gradient descent type:\",GRAD)\n",
    "print(f\"learning rate        = {l_rate_ini} --> {l_rate_fin}\")\n",
    "if gamma!=0: print(f\"gamma={gamma}\\tregularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "w = sigma * np.random.randn(D,L)\n",
    "#a = sigma * np.random.randn(D)\n",
    "# using Hinton initialization of visible biases\n",
    "a = Hinton_bias_init(data)\n",
    "# hidden biases initialized to zero\n",
    "b = np.zeros(L)\n",
    "#print(\"w=\",w);print(\"a=\",a);print(\"b=\",b)\n",
    "\n",
    "# nr of epochs\n",
    "Nepoch=200\n",
    "# minibatches per epoch\n",
    "Nmini=20\n",
    "# minibatch size at initial epoch and final one\n",
    "N_ini,N_fin=10,500\n",
    "print(f\"Nepoch={Nepoch}\\nNmini={Nmini}\")\n",
    "# number of CD steps\n",
    "Nt=2\n",
    "\n",
    "# recording history of weights (\"E\" means epoch)\n",
    "wE,aE,bE=np.zeros((Nepoch+1,D,L)),np.zeros((Nepoch+1,D)),np.zeros((Nepoch+1,L))\n",
    "wE[0],aE[0],bE[0]=np.copy(w),np.copy(a),np.copy(b)\n",
    "gwE,gw2E,gwE_d,gwE_m = np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE)\n",
    "gaE,ga2E,gaE_d,gaE_m = np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE)\n",
    "gbE,gb2E,gbE_d,gbE_m = np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE)\n",
    "miniE = np.zeros(Nepoch+1)\n",
    "pzE=np.zeros((Nepoch+1,Nz))\n",
    "\n",
    "log_L_list = []\n",
    "\n",
    "if GRAD==\"RMSprop\": \n",
    "    gw2,ga2,gb2 = np.zeros_like(w),np.zeros_like(a),np.zeros_like(b)\n",
    "\n",
    "indices=np.arange(Nd).astype(\"int\")\n",
    "plot_weights_bias(wE, aE, 0, L, cols=L//2, save=False)\n",
    "\n",
    "# for the plot with panels\n",
    "Ncols=min(8,max(2,L//2))\n",
    "\n",
    "if POTTS: print(\"Starting the training, POTTS=True\")\n",
    "else: print(\"Starting the training\")\n",
    "\n",
    "# Note: here an epoch does not analyze the whole dataset\n",
    "for epoch in range(1,1+Nepoch):\n",
    "    # q maps epochs to interval [0,1]\n",
    "    q = (epoch-1.)/(Nepoch-1.) \n",
    "    # N, size of the mini batch\n",
    "    # stays closer to N_ini for some time, then it progressively accelerates toward N_fin\n",
    "    N = int(N_ini + (N_fin-N_ini)*(q**2))\n",
    "    #  l_rate interpolates between initial and final value\n",
    "    l_rate = l_rate_ini + (l_rate_fin-l_rate_ini)*q\n",
    "\n",
    "    for mini in range(Nmini):\n",
    "        # initializitation for averages in minibatch\n",
    "        # visible variables \"v\" --> \"x\"\n",
    "        #  hidden variables \"h\" --> \"z\"\n",
    "        x_data, x_model = np.zeros(D),np.zeros(D)\n",
    "        z_data, z_model = np.zeros(L),np.zeros(L)\n",
    "        xz_data,xz_model= np.zeros((D,L)),np.zeros((D,L))\n",
    "        pz = np.zeros(L)\n",
    "        \n",
    "        # Minibatch of size N: points randomply picked (without repetition) from data\n",
    "        selected = np.random.choice(indices,N,replace=False)\n",
    "        if epoch==1 and mini<=3: print(selected)\n",
    "        \n",
    "        for k in range(N):\n",
    "            ###################################\n",
    "            x0 = data[selected[k]]\n",
    "            # positive CD phase: generating z from x[k]\n",
    "            z = CD_step(x0,w,b,POTTS=POTTS)\n",
    "            x_data  += x0\n",
    "            z_data  += z\n",
    "            xz_data += np.outer(x0,z)\n",
    "            # fantasy\n",
    "            zf=np.copy(z)\n",
    "            # Contrastive divergence with Nt steps\n",
    "            for t in range(Nt):\n",
    "                # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "                xf = CD_step(zf,w.T,a)\n",
    "                # positive CD phase: generating fantasy zf from fantasy xf \n",
    "                zf = CD_step(xf,w,b,POTTS=POTTS)\n",
    "            x_model += xf\n",
    "            z_model += zf\n",
    "            xz_model+= np.outer(xf,zf)\n",
    "            # recording probability of encoding in z-space, if POTTS\n",
    "            if POTTS: pz[zf]+=1\n",
    "            ###################################\n",
    "        \n",
    "        # gradient of the likelihood: follow it along its positive direction\n",
    "        gw_d,gw_m = xz_data/N, xz_model/N\n",
    "        ga_d,ga_m = x_data/N, x_model/N\n",
    "        gb_d,gb_m = z_data/N, z_model/N\n",
    "        gw=np.copy(gw_d - gw_m)\n",
    "        ga=np.copy(ga_d - ga_m)\n",
    "        gb=np.copy(gb_d - gb_m)\n",
    "\n",
    "        # gradient ascent step\n",
    "        if GRAD==\"RMSprop\":\n",
    "            # RMSprop gradient ascent\n",
    "            gw2 = beta*gw2+(1-beta)*np.square(gw)\n",
    "            ga2 = beta*ga2+(1-beta)*np.square(ga)\n",
    "            gb2 = beta*gb2+(1-beta)*np.square(gb)\n",
    "            w += l_rate*gw/sqrt(epsilon+gw2)\n",
    "            a += l_rate*ga/sqrt(epsilon+ga2)\n",
    "            b += l_rate*gb/sqrt(epsilon+gb2)\n",
    "        else: \n",
    "            # defaulting to the vanilla stochastic gradient ascent (SGD)\n",
    "            w += l_rate*gw\n",
    "            a += l_rate*ga\n",
    "            b += l_rate*gb\n",
    "        # regularization (LASSO)\n",
    "        if gamma>0.:\n",
    "            w -= (gamma*l_rate)*sign(w)\n",
    "            a -= (gamma*l_rate)*sign(a)\n",
    "            b -= (gamma*l_rate)*sign(b)\n",
    "\n",
    "    wE[epoch],gwE[epoch],gwE_d[epoch],gwE_m[epoch]=np.copy(w),np.copy(gw),np.copy(gw_d),np.copy(gw_m)\n",
    "    aE[epoch],gaE[epoch],gaE_d[epoch],gaE_m[epoch]=np.copy(a),np.copy(ga),np.copy(ga_d),np.copy(ga_m)\n",
    "    bE[epoch],gbE[epoch],gbE_d[epoch],gbE_m[epoch]=np.copy(b),np.copy(gb),np.copy(gb_d),np.copy(gb_m)\n",
    "    miniE[epoch]=N\n",
    "#################################################################################\n",
    "    #all_z = np.array([(x,y,z) for x in [0,1] for y in [0,1] for z in [0,1]])\n",
    "    all_z = list(itertools.product([0, 1], repeat=L))\n",
    "    Z_a = []\n",
    "    exp_data_dip = []\n",
    "    q = 1.4\n",
    "    for z in all_z:\n",
    "        G = np.prod([np.exp(z[i]*b[i]) for i in range(len(b))])\n",
    "        H = a + np.dot(w,z) \n",
    "        #q = np.mean(np.ones(len(H))+np.exp(H))\n",
    "    \n",
    "        Z_tilde = G*np.prod((np.ones(len(H))+np.exp(H))/q)\n",
    "        Z_a.append(Z_tilde)\n",
    "\n",
    "        #Ora voglio ciclare sulle x dei dati (i primi 1000 dati)\n",
    "        s_prod_exp_tot = 0\n",
    "        for ics in data:\n",
    "            s_prod_exp = H.dot(ics)\n",
    "            s_prod_exp_tot += s_prod_exp\n",
    "        exp_data_dip.append(s_prod_exp_tot)\n",
    "\n",
    "    #Z\n",
    "    Z = np.sum(Z_a)\n",
    "    log_Z = np.log(Z)+D*np.log(q)\n",
    "    #log_first_part = np.log(np.sum(np.exp(exp_data_dip)))\n",
    "    log_first_part = scipy.special.logsumexp(exp_data_dip)\n",
    "    \n",
    "    log_L = (log_first_part - log_Z)/D\n",
    "    \n",
    "    if POTTS: pzE[epoch] = pz/np.sum(pz)\n",
    "    print(\"epoch\",epoch,\"/\",Nepoch,\" Nt:\",Nt,\" N:\",N,\" L:\",L,\n",
    "          \" rate:\",l_rate,\" gam:\",gamma,\"SPINS=\",SPINS,\"POTTS=\",POTTS)\n",
    "    print(\"The log-likelihood at this point of the training is \", log_L)\n",
    "\n",
    "    log_L_list.append(log_L)\n",
    "    \n",
    "    if epoch%20==0 or epoch==Nepoch:\n",
    "        plot_weights_bias(wE, aE, epoch, L, cols=Ncols, save=False)\n",
    "            \n",
    "    str_time_completion = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M\")\n",
    "\n",
    "print(\"END of learning phase\")\n",
    "print(f\"Shape of weights: {wE[5].shape}\")\n",
    "\n",
    "plt.plot(range(Nepoch), log_L_list)\n",
    "plt.title(\"log-Likehood trend\")  # Titolo\n",
    "plt.xlabel(\"# epoch\")  # Etichetta asse X\n",
    "plt.ylabel(\"log-L\")  # Etichetta asse Y\n",
    "plt.grid(True)  # Aggiunge una griglia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with the log-likehood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "w = sigma * np.random.randn(D,L)\n",
    "#a = sigma * np.random.randn(D)\n",
    "# using Hinton initialization of visible biases\n",
    "a = Hinton_bias_init(data)\n",
    "# hidden biases initialized to zero\n",
    "b = np.zeros(L)\n",
    "#print(\"w=\",w);print(\"a=\",a);print(\"b=\",b)\n",
    "\n",
    "# nr of epochs\n",
    "Nepoch=200\n",
    "# minibatches per epoch\n",
    "Nmini=20\n",
    "# minibatch size at initial epoch and final one\n",
    "N_ini,N_fin=10,500\n",
    "print(f\"Nepoch={Nepoch}\\nNmini={Nmini}\")\n",
    "# number of CD steps\n",
    "Nt=2\n",
    "\n",
    "# recording history of weights (\"E\" means epoch)\n",
    "wE,aE,bE=np.zeros((Nepoch+1,D,L)),np.zeros((Nepoch+1,D)),np.zeros((Nepoch+1,L))\n",
    "wE[0],aE[0],bE[0]=np.copy(w),np.copy(a),np.copy(b)\n",
    "gwE,gw2E,gwE_d,gwE_m = np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE)\n",
    "gaE,ga2E,gaE_d,gaE_m = np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE)\n",
    "gbE,gb2E,gbE_d,gbE_m = np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE)\n",
    "miniE = np.zeros(Nepoch+1)\n",
    "pzE=np.zeros((Nepoch+1,Nz))\n",
    "\n",
    "log_L_list = []\n",
    "\n",
    "if GRAD==\"RMSprop\": \n",
    "    gw2,ga2,gb2 = np.zeros_like(w),np.zeros_like(a),np.zeros_like(b)\n",
    "\n",
    "indices=np.arange(Nd).astype(\"int\")\n",
    "plot_weights_bias(wE, aE, 0, L, cols=L//2, save=False)\n",
    "\n",
    "# for the plot with panels\n",
    "Ncols=min(8,max(2,L//2))\n",
    "\n",
    "if POTTS: print(\"Starting the training, POTTS=True\")\n",
    "else: print(\"Starting the training\")\n",
    "\n",
    "# Note: here an epoch does not analyze the whole dataset\n",
    "for epoch in range(1,1+Nepoch):\n",
    "    # q maps epochs to interval [0,1]\n",
    "    q = (epoch-1.)/(Nepoch-1.) \n",
    "    # N, size of the mini batch\n",
    "    # stays closer to N_ini for some time, then it progressively accelerates toward N_fin\n",
    "    N = int(N_ini + (N_fin-N_ini)*(q**2))\n",
    "    #  l_rate interpolates between initial and final value\n",
    "    l_rate = l_rate_ini + (l_rate_fin-l_rate_ini)*q\n",
    "\n",
    "    for mini in range(Nmini):\n",
    "        # initializitation for averages in minibatch\n",
    "        # visible variables \"v\" --> \"x\"\n",
    "        #  hidden variables \"h\" --> \"z\"\n",
    "        x_data, x_model = np.zeros(D),np.zeros(D)\n",
    "        z_data, z_model = np.zeros(L),np.zeros(L)\n",
    "        xz_data,xz_model= np.zeros((D,L)),np.zeros((D,L))\n",
    "        pz = np.zeros(L)\n",
    "        \n",
    "        # Minibatch of size N: points randomply picked (without repetition) from data\n",
    "        selected = np.random.choice(indices,N,replace=False)\n",
    "        if epoch==1 and mini<=3: print(selected)\n",
    "        \n",
    "        for k in range(N):\n",
    "            ###################################\n",
    "            x0 = data[selected[k]]\n",
    "            # positive CD phase: generating z from x[k]\n",
    "            z = CD_step(x0,w,b,POTTS=POTTS)\n",
    "            x_data  += x0\n",
    "            z_data  += z\n",
    "            xz_data += np.outer(x0,z)\n",
    "            # fantasy\n",
    "            zf=np.copy(z)\n",
    "            # Contrastive divergence with Nt steps\n",
    "            for t in range(Nt):\n",
    "                # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "                xf = CD_step(zf,w.T,a)\n",
    "                # positive CD phase: generating fantasy zf from fantasy xf \n",
    "                zf = CD_step(xf,w,b,POTTS=POTTS)\n",
    "            x_model += xf\n",
    "            z_model += zf\n",
    "            xz_model+= np.outer(xf,zf)\n",
    "            # recording probability of encoding in z-space, if POTTS\n",
    "            if POTTS: pz[zf]+=1\n",
    "            ###################################\n",
    "        \n",
    "        # gradient of the likelihood: follow it along its positive direction\n",
    "        gw_d,gw_m = xz_data/N, xz_model/N\n",
    "        ga_d,ga_m = x_data/N, x_model/N\n",
    "        gb_d,gb_m = z_data/N, z_model/N\n",
    "        gw=np.copy(gw_d - gw_m)\n",
    "        ga=np.copy(ga_d - ga_m)\n",
    "        gb=np.copy(gb_d - gb_m)\n",
    "\n",
    "        # gradient ascent step\n",
    "        if GRAD==\"RMSprop\":\n",
    "            # RMSprop gradient ascent\n",
    "            gw2 = beta*gw2+(1-beta)*np.square(gw)\n",
    "            ga2 = beta*ga2+(1-beta)*np.square(ga)\n",
    "            gb2 = beta*gb2+(1-beta)*np.square(gb)\n",
    "            w += l_rate*gw/sqrt(epsilon+gw2)\n",
    "            a += l_rate*ga/sqrt(epsilon+ga2)\n",
    "            b += l_rate*gb/sqrt(epsilon+gb2)\n",
    "        else: \n",
    "            # defaulting to the vanilla stochastic gradient ascent (SGD)\n",
    "            w += l_rate*gw\n",
    "            a += l_rate*ga\n",
    "            b += l_rate*gb\n",
    "        # regularization (LASSO)\n",
    "        if gamma>0.:\n",
    "            w -= (gamma*l_rate)*sign(w)\n",
    "            a -= (gamma*l_rate)*sign(a)\n",
    "            b -= (gamma*l_rate)*sign(b)\n",
    "\n",
    "    wE[epoch],gwE[epoch],gwE_d[epoch],gwE_m[epoch]=np.copy(w),np.copy(gw),np.copy(gw_d),np.copy(gw_m)\n",
    "    aE[epoch],gaE[epoch],gaE_d[epoch],gaE_m[epoch]=np.copy(a),np.copy(ga),np.copy(ga_d),np.copy(ga_m)\n",
    "    bE[epoch],gbE[epoch],gbE_d[epoch],gbE_m[epoch]=np.copy(b),np.copy(gb),np.copy(gb_d),np.copy(gb_m)\n",
    "    miniE[epoch]=N\n",
    "#################################################################################\n",
    "    #all_z = np.array([(x,y,z) for x in [0,1] for y in [0,1] for z in [0,1]])\n",
    "    log_L = log_likelihood(a,b,w,data)\n",
    "    \n",
    "    if POTTS: pzE[epoch] = pz/np.sum(pz)\n",
    "    print(\"epoch\",epoch,\"/\",Nepoch,\" Nt:\",Nt,\" N:\",N,\" L:\",L,\n",
    "          \" rate:\",l_rate,\" gam:\",gamma,\"SPINS=\",SPINS,\"POTTS=\",POTTS)\n",
    "    print(\"The log-likelihood at this point of the training is \", log_L)\n",
    "\n",
    "    log_L_list.append(log_L)\n",
    "    \n",
    "    if epoch%20==0 or epoch==Nepoch:\n",
    "        plot_weights_bias(wE, aE, epoch, L, cols=Ncols, save=False)\n",
    "            \n",
    "    str_time_completion = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M\")\n",
    "\n",
    "print(\"END of learning phase\")\n",
    "print(f\"Shape of weights: {wE[5].shape}\")\n",
    "\n",
    "plt.plot(range(Nepoch), log_L_list)\n",
    "plt.title(\"log-Likehood trend\")  # Titolo\n",
    "plt.xlabel(\"# epoch\")  # Etichetta asse X\n",
    "plt.ylabel(\"log-L\")  # Etichetta asse Y\n",
    "plt.grid(True)  # Aggiunge una griglia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(a, b, w, data):\n",
    "    \n",
    "    Z_a = []\n",
    "    exp_data_dip = []\n",
    "    L = len(b)\n",
    "    D = len(data[0])\n",
    "    q = 1.4\n",
    "    all_z = list(itertools.product([0, 1], repeat=L))\n",
    "    for z in all_z:\n",
    "        G = np.prod([np.exp(z[i]*b[i]) for i in range(len(b))])\n",
    "        H = a + np.dot(w,z) \n",
    "        #q = np.mean(np.ones(len(H))+np.exp(H))\n",
    "    \n",
    "        Z_tilde = G*np.prod((np.ones(len(H))+np.exp(H))/q)\n",
    "        Z_a.append(Z_tilde)\n",
    "\n",
    "        #Ora voglio ciclare sulle x dei dati (i primi 1000 dati)\n",
    "        s_prod_exp_tot = 0\n",
    "        for ics in data:\n",
    "            s_prod_exp = H.dot(ics)\n",
    "            s_prod_exp_tot += s_prod_exp\n",
    "        exp_data_dip.append(s_prod_exp_tot)\n",
    "\n",
    "    #Z\n",
    "    Z = np.sum(Z_a)\n",
    "    log_Z = np.log(Z)+D*np.log(q)\n",
    "    #log_first_part = np.log(np.sum(np.exp(exp_data_dip)))\n",
    "    log_first_part = scipy.special.logsumexp(exp_data_dip)\n",
    "    \n",
    "    log_L = (log_first_part - log_Z)/D\n",
    "\n",
    "    return log_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test generative power of the trained RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee=-1\n",
    "NN=200\n",
    "traj_x,traj_z = np.zeros((NN+2,D)), np.zeros((NN+2,L))\n",
    "xf=np.copy(data[np.random.randint(Nd)])\n",
    "traj_x[0]=np.copy(xf)\n",
    "\n",
    "# AF: multiply weights and biases by a number >1 to achieve a more deterministic behavior,\n",
    "# equivalent to lower the temperature in a Boltzmann weight -> select lowest energies\n",
    "# Note: here, this is done in the negative CD step only\n",
    "AF=1.\n",
    "\n",
    "for t in range(NN):\n",
    "    t1=t+1\n",
    "    # positive CD phase: generating fantasy zf from fantasy xf \n",
    "    zf = CD_step(xf,1*wE[ee],1*bE[ee],POTTS=POTTS)\n",
    "    traj_z[t] = np.copy(zf)\n",
    "    # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "    xf = CD_step(zf,AF*wE[ee].T,AF*aE[ee])\n",
    "    traj_x[t1] = np.copy(xf)\n",
    "\n",
    "\n",
    "plot_weights_bias(wE, aE, Nepoch, L, cols=Ncols, save=False)\n",
    "\n",
    "col0,col1,col2,col3,col4=(0.8,0,0,1),(1,0.6,0,1),(0,.7,0,1),(0.2,0.2,1,1),(1,0,1,1)\n",
    "show_MNIST(traj_x[0:],Nex=10,colors=(col0,col1))\n",
    "show_MNIST(traj_x[10:],Nex=10,colors=(col1,col2))\n",
    "show_MNIST(traj_x[20:],Nex=10,colors=(col2,col3))\n",
    "show_MNIST(traj_x[40:],Nex=10,colors=(col3,col4))\n",
    "print(\"L:\",L,\"    amplification of weights:\",AF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the gradient amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fname(strin):\n",
    "    fname=\"FIG/RBM_\"+strin+\"_MNIST\"+str(Ndigit)+\"_E\"+str(NE)+\"_N\"+str(N_ini)+\"-\"+str(N_fin)\n",
    "    fname=fname+\"_\"+GRAD+\"-\"+str(l_rate_ini)+\"-\"+str(l_rate_fin)+\"_CD-\"+str(Nt)+\"_L\"+str(L)\n",
    "    if gamma>0:  fname=fname+\"_reg\"+str(gamma)\n",
    "    return fname+\".png\"\n",
    "\n",
    "NE = len(gwE)-1\n",
    "print(\"NE=\",NE)\n",
    "mgw,mga,mgb,epo=np.zeros(NE),np.zeros(NE),np.zeros(NE),np.zeros(NE)\n",
    "DE=1\n",
    "if NE>30: DE=2\n",
    "if NE>50: DE=5\n",
    "if NE>100: DE=10\n",
    "# compute RMS average of gradients during epochs\n",
    "for ep in range(1,1+NE):\n",
    "    epo[ep-1]=ep\n",
    "    mgw[ep-1] = np.std(gwE[ep])\n",
    "    mga[ep-1] = np.std(gaE[ep])\n",
    "    mgb[ep-1] = np.std(gbE[ep])\n",
    "\n",
    "#############################################################################\n",
    "# creating grid for subplots\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,5.5))\n",
    "fig.subplots_adjust( left=None, bottom=None,  right=None, top=None, wspace=None, hspace=None)\n",
    "        \n",
    "ax.xaxis.set_major_locator(mpl.ticker.MultipleLocator(DE))\n",
    "ax.plot(epo,mgw,\"o-\",c=\"#CC77EC\",label=\"$w$\",zorder=10)\n",
    "ax.plot(epo,mga,\"d--\",c=\"#22C040\",label=\"$a$\",zorder=5)\n",
    "ax.plot(epo,mgb,\"^:\",c=\"#EEC006\",label=\"$b$\",zorder=8)\n",
    "ax.plot(epo,.35/sqrt(miniE[1:]),\"k-.\",label=\"$\\sim 1 / \\sqrt{N}$\",zorder=20,lw=2)\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"gradient RMS\")\n",
    "ax.set_xlim(0.5,NE+0.5)\n",
    "if 1==2: ax.set_ylim(0,)\n",
    "else: \n",
    "    ax.set_xlim(0.8,Nepoch*1.1)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "\n",
    "fname=make_fname(\"grad\")\n",
    "fig.savefig(fname,dpi=200,bbox_inches='tight',pad_inches=0.02, transparent=False)\n",
    "plt.show()\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights over epochs on file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_fname(label,dname=\"WEIGHTS\"):\n",
    "    fname=dname+\"/\"+label+\"_\"+str_simul+\"_MNIST\"+str(Ndigit)+\"_L\"+str(L)\n",
    "    fname=fname+\"_\"+GRAD+\"-\"+\"{:.2f}\".format(l_rate_ini)+\"-\"+\"{:.2f}\".format(l_rate_fin)+\"_Ep\"+str(Nepoch)\n",
    "    if gamma>0:  fname=fname+\"_reg\"+\"{:.3g}\".format(gamma)\n",
    "    if POTTS: fname+=\"_POTTS\"\n",
    "    if SPINS: fname+=\"_SPINS\"\n",
    "    fname=fname+str_time_completion\n",
    "    return fname+\".npy\"\n",
    "\n",
    "def save_epochs(qE,label):\n",
    "    fname=make_weights_fname(label)\n",
    "    print(fname)\n",
    "    np.save(fname,qE)\n",
    "\n",
    "save_epochs(wE,\"w\")\n",
    "save_epochs(bE,\"b\")\n",
    "save_epochs(aE,\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.       Log-likelihood $\\mathcal{L}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) plot the log-likelihood $\\mathcal{L}$ at $L=3$ during the training (as function of epochs)\n",
    "2) verify dependance of $\\mathcal{L}$ on the number of CD steps\n",
    "3) compare $\\mathcal(L)$ for $L = 2, 3, 4, ...$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy function for computing the log_l\n",
    "# x and z are vectors of size D and L respectively, w has shape (D, L), a is (D) and b is (L)\n",
    "\n",
    "def E(x: np.array, z: np.array, w: np.ndarray, a: np.array, b: np.array):\n",
    "    E = -a.dot(x) - b.dot(z) - x.dot(w.dot(b))\n",
    "    return E\n",
    "\n",
    "#print(E(data[1], np.array([1]*L), wE[3], aE[3], bE[3]))\n",
    "\n",
    "# G(z) and P(z) are used later to compute log_l\n",
    "\n",
    "def G(z: np.array, b: np.array):\n",
    "    return exp(b.dot(z))\n",
    "\n",
    "def P(z: np.array, w: np.ndarray, a:np.array):\n",
    "    P = 1\n",
    "    for i in range(w.shape[0]):\n",
    "        P *= 1 + exp(w[i, :].dot(z) + a[i]) \n",
    "    return P\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# log_l is the log-likelihood for a --single-- sample x^(m) (\\mathcal{L} is the average on m of log_l)\n",
    "\n",
    "def log_l(x:np.array, w:np.ndarray, a: np.array, b:np.array):\n",
    "    # log_l is the diff between the log-sum of the activations (exp of energies) and the log-partition function\n",
    "\n",
    "    hidden_states = list(it.product((0, 1), repeat=L))\n",
    "    energies = np.array([E(x, z, w, a, b) for z in hidden_states])\n",
    "    # print(hidden_states)\n",
    "    # print(f'shape of energies: {energies.shape}')\n",
    "    log_sum = -log( np.sum(exp(-energies)) )\n",
    "\n",
    "    log_Z = -log(np.sum ([G(z, b)*P(z, w, a) for z in hidden_states]) )\n",
    "    log_l = log_sum + log_Z\n",
    "    return log_l\n",
    "\n",
    "def Likelihood(data: np.ndarray, w: np.ndarray, a: np.array, b: np.array):\n",
    "    logs = np.array( [log_l(data[i, :], w, a, b) for i in range(data.shape[0])] )\n",
    "    return np.mean(logs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RBM(L, Nt, Nepoch = 100, GRAD = \"RMSprop\"): \n",
    "\n",
    "    # minibatches per epoch\n",
    "    Nmini=20\n",
    "    # minibatch size at initial epoch and final one\n",
    "    N_ini,N_fin=10,500\n",
    "\n",
    "    # initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "    w = sigma * np.random.randn(D,L)\n",
    "    #a = sigma * np.random.randn(D)\n",
    "    # using Hinton initialization of visible biases\n",
    "    a = Hinton_bias_init(data)\n",
    "    # hidden biases initialized to zero\n",
    "    b = np.zeros(L)\n",
    "\n",
    "    print(f\"Nepoch={Nepoch}\\nNmini={Nmini}\")\n",
    "\n",
    "    # recording history of weights (\"E\" means epoch)\n",
    "    wE,aE,bE=np.zeros((Nepoch+1,D,L)),np.zeros((Nepoch+1,D)),np.zeros((Nepoch+1,L))\n",
    "    wE[0],aE[0],bE[0]=np.copy(w),np.copy(a),np.copy(b)\n",
    "    gwE,gw2E,gwE_d,gwE_m = np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE),np.zeros_like(wE)\n",
    "    gaE,ga2E,gaE_d,gaE_m = np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE),np.zeros_like(aE)\n",
    "    gbE,gb2E,gbE_d,gbE_m = np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE),np.zeros_like(bE)\n",
    "    miniE = np.zeros(Nepoch+1)\n",
    "\n",
    "    logL_E = np.zeros(Nepoch+1)\n",
    "\n",
    "    pzE=np.zeros((Nepoch+1,Nz))\n",
    "    if GRAD==\"RMSprop\": \n",
    "        gw2,ga2,gb2 = np.zeros_like(w),np.zeros_like(a),np.zeros_like(b)\n",
    "\n",
    "    indices=np.arange(Nd).astype(\"int\")\n",
    "    plot_weights_bias(wE, aE, 0, L, cols=L//2, save=False)\n",
    "\n",
    "    # for the plot with panels\n",
    "    Ncols=min(8,max(2,L//2))\n",
    "\n",
    "    if POTTS: print(\"Starting the training, POTTS=True\")\n",
    "    else: print(\"Starting the training\")\n",
    "\n",
    "    # Note: here an epoch does not analyze the whole dataset\n",
    "    for epoch in range(1,Nepoch+1):\n",
    "        # q maps epochs to interval [0,1]\n",
    "        q = (epoch-1.)/(Nepoch-1.) \n",
    "        # N, size of the mini batch\n",
    "        # stays closer to N_ini for some time, then it progressively accelerates toward N_fin\n",
    "        N = int(N_ini + (N_fin-N_ini)*(q**2))\n",
    "        #  l_rate interpolates between initial and final value\n",
    "        l_rate = l_rate_ini + (l_rate_fin-l_rate_ini)*q\n",
    "\n",
    "        selected = np.random.choice(indices,N,replace=False)\n",
    "\n",
    "        for mini in range(Nmini):\n",
    "            # initializitation for averages in minibatch\n",
    "            # visible variables \"v\" --> \"x\"\n",
    "            #  hidden variables \"h\" --> \"z\"\n",
    "            x_data, x_model = np.zeros(D),np.zeros(D)\n",
    "            z_data, z_model = np.zeros(L),np.zeros(L)\n",
    "            xz_data,xz_model= np.zeros((D,L)),np.zeros((D,L))\n",
    "            pz = np.zeros(L)\n",
    "            \n",
    "            # Minibatch of size N: points randomply picked (without repetition) from data\n",
    "            if epoch==1 and mini<=3: print(selected)\n",
    "\n",
    "            for k in range(N):\n",
    "                ###################################\n",
    "                x0 = data[selected[k]]\n",
    "                # positive CD phase: generating z from x[k]\n",
    "                z = CD_step(x0,w,b,POTTS=POTTS)\n",
    "                x_data  += x0\n",
    "                z_data  += z\n",
    "                xz_data += np.outer(x0,z)\n",
    "                # fantasy\n",
    "                zf=np.copy(z)\n",
    "                # Contrastive divergence with Nt steps\n",
    "                for t in range(Nt):\n",
    "                    # negative CD pzase: generating fantasy xf from fantasy zf\n",
    "                    xf = CD_step(zf,w.T,a)\n",
    "                    # positive CD phase: generating fantasy zf from fantasy xf \n",
    "                    zf = CD_step(xf,w,b,POTTS=POTTS)\n",
    "                x_model += xf\n",
    "                z_model += zf\n",
    "                xz_model+= np.outer(xf,zf)\n",
    "                # recording probability of encoding in z-space, if POTTS\n",
    "                if POTTS: pz[zf]+=1\n",
    "                ###################################\n",
    "            \n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            gw_d,gw_m = xz_data/N, xz_model/N\n",
    "            ga_d,ga_m = x_data/N, x_model/N\n",
    "            gb_d,gb_m = z_data/N, z_model/N\n",
    "            gw=np.copy(gw_d - gw_m)\n",
    "            ga=np.copy(ga_d - ga_m)\n",
    "            gb=np.copy(gb_d - gb_m)\n",
    "\n",
    "            # gradient ascent step\n",
    "            if GRAD==\"RMSprop\":\n",
    "                # RMSprop gradient ascent\n",
    "                gw2 = beta*gw2+(1-beta)*np.square(gw)\n",
    "                ga2 = beta*ga2+(1-beta)*np.square(ga)\n",
    "                gb2 = beta*gb2+(1-beta)*np.square(gb)\n",
    "                w += l_rate*gw/sqrt(epsilon+gw2)\n",
    "                a += l_rate*ga/sqrt(epsilon+ga2)\n",
    "                b += l_rate*gb/sqrt(epsilon+gb2)\n",
    "            else: \n",
    "                # defaulting to the vanilla stochastic gradient ascent (SGD)\n",
    "                w += l_rate*gw\n",
    "                a += l_rate*ga\n",
    "                b += l_rate*gb\n",
    "            # regularization (LASSO)\n",
    "            if gamma>0.:\n",
    "                w -= (gamma*l_rate)*sign(w)\n",
    "                a -= (gamma*l_rate)*sign(a)\n",
    "                b -= (gamma*l_rate)*sign(b)\n",
    "\n",
    "        wE[epoch],gwE[epoch],gwE_d[epoch],gwE_m[epoch]=np.copy(w),np.copy(gw),np.copy(gw_d),np.copy(gw_m)\n",
    "        aE[epoch],gaE[epoch],gaE_d[epoch],gaE_m[epoch]=np.copy(a),np.copy(ga),np.copy(ga_d),np.copy(ga_m)\n",
    "        bE[epoch],gbE[epoch],gbE_d[epoch],gbE_m[epoch]=np.copy(b),np.copy(gb),np.copy(gb_d),np.copy(gb_m)\n",
    "        miniE[epoch]=N\n",
    "\n",
    "        all_z = np.array([(x,y,z) for x in [0,1] for y in [0,1] for z in [0,1]])\n",
    "        Z_a = []\n",
    "        ex_l = []\n",
    "        for z in all_z:\n",
    "            G = np.prod([np.exp(z[i]*b[i]) for i in range(len(b))])\n",
    "            H = a + np.dot(w,z) \n",
    "            q = np.mean(np.ones(len(H))+np.exp(H))\n",
    "        \n",
    "            Z_tilde = G*np.prod((np.ones(len(H))+np.exp(H))/q)\n",
    "            Z_a.append(Z_tilde)\n",
    "            \n",
    "            v = np.dot(H, x_model)\n",
    "            ex_l.append(v)\n",
    "    \n",
    "            Z = q**D*np.sum(Z_a)\n",
    "            log_Z = np.log(Z)\n",
    "        \n",
    "        log_likelihoods = np.log(np.sum(ex_l))\n",
    "        \n",
    "        log_L = np.mean(log_likelihoods - log_Z)\n",
    "        \n",
    "        if POTTS: pzE[epoch] = pz/np.sum(pz)\n",
    "        print(\"epoch\",epoch,\"/\",Nepoch,\" Nt:\",Nt,\" N:\",N,\" L:\",L,\n",
    "              \" rate:\",l_rate,\" gam:\",gamma,\"SPINS=\",SPINS,\"POTTS=\",POTTS)\n",
    "        print(\"The log-likelihood at this point of the training is \", log_L/D)\n",
    "    \n",
    "        if Nepoch<=100 or epoch%20==0 or epoch==Nepoch:\n",
    "            plot_weights_bias(wE, aE, epoch, L, cols=Ncols, save=False)\n",
    "                \n",
    "        str_time_completion = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M\")\n",
    "    \n",
    "    print(\"END of learning phase\")\n",
    "\n",
    "\n",
    "train_RBM(L, Nt, Nepoch = 100, GRAD = \"RMSprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(Nepoch+1), loglikelihoods)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a_test = np.load('WEIGHTS/a_RBM_MNIST3_L4_RMSprop-0.05-0.05_Ep50_reg0.001_20250310_2226.npy')\n",
    "b_test = np.load('WEIGHTS/b_RBM_MNIST3_L4_RMSprop-0.05-0.05_Ep50_reg0.001_20250310_2226.npy')\n",
    "w_test = np.load('WEIGHTS/w_RBM_MNIST3_L4_RMSprop-0.05-0.05_Ep50_reg0.001_20250310_2226.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
